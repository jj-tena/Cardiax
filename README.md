# Cardiax

## Introduction
Cardiovascular Diseases (CVDs) involve a group of disorders affecting the heart and blood vessels, and due to the importance of their points of involvement, they are the leading cause of mortality in the world, with an estimated 20.5 million deaths per year. There are multiple risk factors in the development of CVD, ranging from biological aspects such as diabetes, hypertension, age, sex, family history, to others related to the patient's lifestyle such as alcohol consumption, smoking, poor diet or lack of physical exercise.
Machine learning (ML) is a field of artificial intelligence (AI) that allows training models with large data sets, extracting complex patterns and linear and nonlinear relationships, thus allowing predictions based on data distributions of previous samples and identifying the most relevant features for it. 
Due to the importance in terms of mortality and magnitude of CVD, the use of AI and ML will be studied for the construction of predictive models that evaluate large clinical datasets, find patterns among risk factors and possess the ability to predict the occurrence of future complications, thus allowing the identification of individuals at higher risk of CVD, thus aiding in the planning of preventive interventions.
CVD can go unnoticed in the daily life of those affected by CVD, but without proper attention and treatment it can lead to different types of sudden death events such as: myocardial infarction (heart attack), cerebrovascular accident (CVA) or stroke, heart failure, cardiac arrhythmias, peripheral arterial disease, sudden cardiac death, aortic aneurysm, deep vein thrombosis or pulmonary embolism, among others. 
Faced with this type of extreme mortality events, it is essential to monitor patients to assess the progress of their state of health over time and to have the ability to prevent these events as far in advance as possible. To this end, medicine has developed multiple ways of calculating risk, based on mathematical formulas, ad hoc biomedical devices and values obtained from blood tests. However, the full potential of AI as an ally in the prevention and detection of possible mortality events due to CVD remains unexploited. It is precisely this that has motivated the development of this scientific-academic research.
With this purpose, the Cardiax project was born, which is framed within the field of Biomedical Engineering, particularly in the prevention of CVDs. The objective is to create a web platform that allows the prediction of possible complications derived from CVD by means of an AI model, which will receive as input a series of data on the patient's lifestyle and clinical aspects obtained from analytical tests.

## Objectives
The main objective of this work is the application of different AI techniques for the prognosis and detection of future complications that patients with CVD may suffer.
The main objective is supported by a series of specific objectives (SOs), more concrete and smaller in size, whose completeness makes it possible to achieve the main purpose. 
The list of SOs respective to both the web platform and the development of the AI model is defined below:
[SO1] Evaluate AI predictive models based on transformers optimized for working with tabular datasets.
[SO2] Evaluate the effectiveness and robustness of GANs models for generating synthetic data samples associated with CVDs to address the imbalance problem in classification scenarios.
[SO3] To jointly study post-hoc interpretability techniques and predictive ML models for the identification of risk factors in CVDs. 
[SO4] Build a web platform where users can enter their clinical data and lifestyle habits to obtain a prediction of their risk of developing CVD using the models created in SO1.
[SO5] Allow the viewing of the history of analytical data entered into the system, as well as the prediction associated with each one of them, being able to sort and filter them based on different criteria.
[SO6] Visualize through the use of graphs the temporal evolution of the risk associated with the different analytics entered in the system

## Web application
Cardiax requires a web platform through which users can communicate with the AI model. To build it, it is necessary to develop a frontend that offers an interface with which users can interact, a backend that responds to requests from the frontend and integrates the model, a database that stores user information and the definition of a communication protocol between all the parties involved. This section will explain in more detail the process of defining the design and creation of each of the entities mentioned above.

### Backend
The backend of a web application is the code responsible for managing business logic, responding to requests from the frontend that the user interacts with on their device, and generally performing all server-side operations that are not visible to the user. Specifically, the backend of Cardiax can be found at the following link: https://github.com/jj-tena/Cardiax/tree/main/backend.
Django is a backend web development framework, which means it is a set of tools that facilitates the development of software running on a server, designed to work with the Python programming language.
To run the project locally, open a terminal in the backend folder and execute the following list of commands:
* python -m venv venv: Creates the Python virtual environment.
* venv\Scripts\activate: Activates the Python virtual environment for use.
* python3 manage.py makemigrations: Creates a migration file with the project’s code files.
* python3 manage.py migrate: Deploys the migration file in the virtual environment.
* python manage.py runserver: Runs a local server with the project on port 8000 (To access: http://localhost:8000).
Django https://www.djangoproject.com was primarily chosen as the framework to develop the platform's backend because of its native compatibility with Python, allowing direct integration of the AI model developed in the previous section using this language.
Specifically, Django Rest Framework https://www.django-rest-framework.org was used, which is an extension of Django that facilitates the creation of RESTful API architectures. It allows the definition of API endpoints to expose the AI model's predictive capabilities and other operations such as user authentication, making them easily consumable by the frontend, which will be discussed more specifically in the Communication section.
Additionally, Django provides its own Object-Relational Mapping (ORM) to interact with databases, a key point when implementing information persistence. These aspects will be detailed further in the Database section.
Regarding authentication, the backend offers methods for users to log in and register, which generate an access token (this will be discussed in more detail in the Communication section) to verify the identity of the user making requests and ensure proper access to their private resources. In terms of storing passwords securely, when a new user registers, their password is always stored in an encoded (hashed) format, and when they want to log in, the hash of the entered password is compared to the stored hash in the database.
For analytics, the backend allows users to add a new one to evaluate their heart attack risk, view the historical list of analytics they have submitted, and obtain a chronologically ordered list to track their cardiovascular health progression.
To integrate the AI, the backend project contains the dataset used to train the model and a ia.py file that ensures the model is trained with the data and evaluated when the server is started, so this process does not have to be repeated every time its services are needed. Additionally, the model offers a method to obtain predictions for new data samples provided by the user.
This allows users to track the temporal evolution of the risk associated with the various analytics entered into the system.

### Frontend
The frontend of a web application is the code that runs directly on users' devices and allows them to interact with the application. It encompasses everything the user can see and interact with in their browser. The frontend of Cardiax is available at the following link: https://github.com/jj-tena/Cardiax/tree/main/frontend.
React was used to create the frontend, which is a JavaScript library developed by Facebook. It allows the development of frontend projects to build the system interface that users interact with. React focuses on the use of components, which are independent, reusable pieces of code that encapsulate interface elements and logic, and views, which are what we traditionally know as complete pages and can consist of multiple components. In the case of Cardiax, the project was programmed using TypeScript to take advantage of its strong typing and avoid potential errors caused by type ambiguity.
To run the frontend project, open a terminal in the frontend folder, run the command npm i to install the project's dependencies, and execute npm start to start the local server at http://localhost:3000.
Material UI is a React component library that implements Google’s Material Design system. It provides a collection of pre-built components that allow the implementation of any visual resource needed for a web frontend. The decision to use Material UI components was made to maintain a professional and consistent design across all parts of the interface.
Responsive design is a web design technique that allows a webpage to automatically adapt to the screen size of the user's device. A responsive design approach was followed in the construction of the Cardiax frontend to ensure that users can access system services and enjoy an optimal web experience, regardless of their device's screen width.
For session persistence, the access token returned by the backend upon successful registration or login is stored in the user's browser's local storage, allowing it to persist even if the user closes the application tab. However, if the user logs out, the token is removed from local storage.

### Communication
Communication between the backend and frontend is carried out through a communication protocol known as REST API (Representational State Transfer), which involves the exchange of data via the HTTP protocol. This architecture allows for a clear separation between the backend and frontend, making the code easier to maintain and enhancing the scalability of the project. Specifically:
The backend exposes a series of endpoints (specific URLs) that represent resources, offering the ability to retrieve information or perform operations on them. Upon receiving requests, the backend processes them and returns an HTTP response, which includes a status code and data in JSON format as a response.
The frontend sends HTTP requests (GET, POST, PUT, DELETE) to consume these services and receives the responses from the backend, which it uses to update the interface, allowing the user to view the results of the operation.
JSON Web Token (JWT) is a standard that allows the creation of tokens capable of storing encrypted information using a secret key. These tokens can be used for user authentication and ensure secure information exchange between the frontend and backend. A JWT consists of a header (token type and encryption algorithm), a payload (information to be stored encrypted), and a signature (a digital signature created by combining the header, payload, and secret key, used to verify that the token has not been altered). The digital signature ensures that the token has not been tampered with, as only the server can generate valid tokens and verify their authenticity using the secret key.
As mentioned in the Backend section, when the frontend submits a login or registration request, the backend responds with a JWT. As explained in the Frontend section, the frontend stores this token in the local storage of the user's browser while their session is active. Every time the frontend makes a request to the backend that requires authentication of resources, it sends the token in the request body. This way, both parts can maintain communication, ensuring that the user's session and the resources they can access are always under control.

### Database
Databases are systems that allow for the storage, management, and querying of information in a structured manner, enabling the implementation of information persistence for applications. The Cardiax database can be viewed via the following link: https://github.com/jj-tena/Cardiax/tree/main/database.
Relational databases are a type of database that organizes data into tables, which are composed of rows and columns. Each table represents an entity, and the relationships between tables are managed through primary and foreign keys.
PostgreSQL [https://www.postgresql.org] is a relational database management system capable of handling large volumes of data and complex queries, making it suitable for web applications aiming to scale in terms of data and traffic.
Docker [https://www.docker.com] is a containerization platform that allows applications and all their dependencies to be packaged into a container, ensuring that the application runs in isolation regardless of the environment in which the container is executed, as long as the system supports Docker. A Docker container is an instance of a Docker image, which includes everything needed to run an application. Additionally, docker-compose.yml files enable quick configuration and deployment of the application within the container with the desired settings, without the need for manual installation.
For Cardiax, a relational database is implemented due to the relationship between users and their analytics. PostgreSQL is specifically used for its performance and scalability, and this database runs in a Docker container for its ease in quickly setting up the database.
Technically, to implement this idea, it was necessary to install Docker on my operating system, which, being Linux, provides many advantages. After that, a docker-compose.yml file was created to download the official PostgreSQL image.
The file allows configuration of the database name, the administrator’s user ID and password, mounting a volume to ensure data persistence between container restarts, and mapping the port on which the database runs inside the container to the port the backend can connect to from outside the container. Finally, with all configurations in place, the database can be accessed from the backend using the container’s IP address and port.
To activate the database, open a terminal in the database folder and run the command sudo docker compose up to make the database available on port 5432. 
To stop the container, run sudo docker compose down.
There are two data models stored in the PostgreSQL database: one for registered user information and the other for the analytics data the user submits.

## Artificial Intelligence Model
The creation of the AI models was carried out using the Google Colab development environment, which allows for the editing of files with the .ipynb extension. These types of files, known as notebooks, combine text cells for project documentation with code cells for executing programming snippets written in the Python language.
The content of the notebook is divided into a series of sections. First, all the datasets to be studied in the project are imported. Additionally, for each dataset, an Exploratory Data Analysis (EDA) is performed, evaluating the content and conducting preprocessing to prepare the data for use in subsequent steps. An important aspect will be the application of undersampling techniques to correct potential imbalances between a high number of patients without CVD risk and a smaller number with such risk.
Once the EDA is completed, the training of AI models begins. These models will learn separately from each dataset using the DT, XGBoost, KNN, and SVM algorithms. Different evaluations of the models will then be conducted to obtain a benchmark with which to compare the innovation techniques and methods intended to surpass these scores.
At this point, techniques whose use we want to study will be applied. Initially, to correct the imbalance, GAN models will be tested, specifically the CTGAN model [Xi2019] for each dataset, with a focus on how to add samples to these datasets using the imbalanced ratio metric.
After correcting the dataset imbalances using GAN networks, a model will be trained for each dataset using the transformer-based technique called TabPFN [Hu2023]. This aims to train with new techniques whose performance is desired to be studied. After this training, the different evaluation metrics used in the previous models will be applied to observe the new performance achieved and compare it with the previous benchmark.
Finally, interpretability techniques SHAP and LIME will be applied to study which features have been most important for the transformer models trained with datasets whose imbalance has been corrected using GAN networks. This will allow for a sensible and reasonable explanation of the predictions made by the models, as well as identify which advice and aspects patients with these types of diseases should pay more attention to.

### Data ingestion
Due to the difficulty in obtaining original data samples because of their private nature, public data sources with verified quality will be used. Below is the list of datasets (DSs) that will be employed:
* [DS1] Statlog (Heart): A dataset with 13 features and 270 samples that allows predicting the absence (0) or presence (1) of heart disease. It belongs to the UCI Machine Learning Repository collection.
* [DS2] Cardiovascular Disease Dataset: A dataset with 12 features and 1000 samples that allows predicting the absence (0) or presence (1) of heart disease. It comes from one of the multispecialty hospitals in India.
* [DS3] Heart Disease Dataset (Comprehensive): A dataset with 11 features and 1190 samples that allows predicting the absence (0) or presence (1) of coronary artery disease. It includes 126 articles and 68 datasets extracted from scientific literature from 1992 to 2018.
* [DS4] Framingham Heart Study Dataset: A dataset with 15 features and 4240 samples that allows predicting the absence (0) or presence (1) of the risk of developing coronary heart disease within 10 years. The Framingham study began in 1948 under the U.S. Public Health Service and was transferred to the National Heart Institute in 1949. It involved men and women from the town of Framingham in Massachusetts.
* [DS5] Heart Disease Health Indicators Dataset: A dataset with 21 features and 253680 samples that allows predicting the absence (0) or presence (1) of heart disease. The Behavioral Risk Factor Surveillance System (BRFSS) is a telephone survey that collects responses from over 400,000 Americans each year on health-related aspects. This dataset is a processed sample from the 2015 edition.
* [DS6] Cardiovascular Diseases Risk Prediction Dataset: A dataset with 18 features and 308854 samples that allows predicting the absence (0) or presence (1) of heart disease. Similarly to the previous dataset, this one comes from the BRFSS survey but is from the 2021 edition.
All these datasets were uploaded to Google Drive in a public repository so that they could be downloaded directly from the notebook without needing to keep local copies. This way, all the information became available in the code execution environment, after which it was transformed into a tabular DataFrame structure using the Pandas library.

### Exploratory data analysis
Once the data acquisition was completed, an exploratory data analysis was conducted to gain a deeper understanding of the dataset to be worked with. The exploratory data analysis will utilize the EDAModule that I developed, from which the ExploratoryDataAnalysis class will be imported with all the necessary functionalities to study the structure and distribution of the information. Therefore, the first step will be to download this module from a public repository located on GitHub, import its class, and then apply its methods to each of the datasets imported in the previous section.
The imported class allows studying the following aspects of each dataset:
* Dimensions: Number of rows and columns in the DataFrame, as well as the total number of data points.
* Variables: List of columns in the DataFrame.
* Types: Type of each variable, primarily whether they are numeric or categorical.
* Unique Values: Number of distinct values stored by each variable and the proportion of these different values.
* Statistics: Various statistics about the distribution of data in the different numeric variables of the DataFrame.
* Null Values: Missing data in any row or column; the absence of data can pose a problem in AI algorithms and data science analysis when understanding variable distributions.
* Outliers: Boxplot charts allow for the observation of value distribution across variables. At this point, they will be used to identify variables with values that are significantly distant from the main distribution. These extremely distant values from the mean are known as outliers.
* Histograms: Histograms are a type of graph that provides a clearer view of the distribution of data for a variable. At this stage, a histogram will be generated for each numeric variable present in the datasets.

### Cleaning and preprocessing
The process of cleaning and preprocessing a dataset aims to prepare it for use in analysis and modeling algorithms. It consists of a series of steps designed to ensure data quality for subsequent manipulation by machine learning algorithms or other techniques.
Data Cleaning involves identifying and correcting potential issues in the dataset to ensure completeness and consistency, which is crucial for obtaining meaningful results in the subsequent analysis. In this work, the detection and removal of outliers and the handling or imputation of missing values will be evaluated.
Data Preprocessing involves a series of transformations and adjustments intended to prepare the data by optimizing its quality and utility specifically for use in modeling algorithms. This work will evaluate feature selection, standardization of numeric variables, and encoding of categorical variables.
Outlier Detection and Removal is a process where unusual or extreme observations in a dataset are identified, and measures are taken to either remove or correct them. Outliers can distort analysis results and negatively impact the performance of machine learning models.
Given the focus on using unsupervised learning techniques in this work, two such algorithms will be used for outlier detection, which will identify potential anomalies without the need for training on labeled samples. The algorithms to be used are:
* Local Outlier Factor (LOF): Based on the idea that outliers have a significantly lower local density compared to their neighbors. LOF calculates an outlier factor for each instance based on the local density of its neighbors; if a point has a high LOF factor, it means it is less densely surrounded than most points. Outliers are identified as those points with an LOF factor significantly greater than 1.
* Isolation Forest: Based on the use of decision trees, it differs from the previous method in that it does not assume that outliers are necessarily in low-density areas. The algorithm builds multiple decision trees randomly, each tree recursively splits the feature space into subsets, randomly selecting a feature and a split value for each division. Outliers are identified more quickly during the tree construction process, as they require fewer splits to be isolated. Anomaly scores are calculated based on the average depth of the nodes where the data points are located; outliers will have lower anomaly scores.
After running both algorithms, the number of outliers detected by each will be recorded, and those detected by both will be removed.
Handling or Imputing Missing Values is a process used in data analysis to address observations with absent values. When missing values are found in a dataset, they must be addressed to avoid negatively affecting the training and evaluation of the model.
Standardization of Numeric Variables aims to transform numeric variables so that they have a mean of zero and a standard deviation of one. This means that the values of the transformed variables will be centered around zero and have similar dispersion, which facilitates comparison and analysis of the variables. Standardization is useful when variables have different scales and units of measurement, as it helps prevent variables with larger magnitudes from dominating the model's contribution.
To perform this process, the StandardScaler algorithm will be used, which standardizes features by scaling them to have a mean of zero and a standard deviation of one.
Encoding Categorical Variables is a process used to convert categorical variables into a numerical form that machine learning algorithms can understand and utilize effectively.
Once all the points to be addressed in the preprocessing and cleaning stage are defined, a method will be created to apply all the above steps consecutively from a single access point.
Undersampling is a technique used in imbalanced classification problems to reduce the number of samples from the majority classes (or more frequent classes) to match the number of samples from the minority classes (or less frequent classes). This is achieved by removing samples from the majority classes until the class proportions are more balanced.
When working with imbalanced datasets, where one class is much more frequent than the others, machine learning models tend to struggle with learning and generalizing correctly about the minority classes. This can lead to biased models that predominantly predict the majority class, ignoring the minority classes. Therefore, it is important to address this class imbalance to obtain more balanced and accurate models.
By reducing the number of samples from the majority classes, the effects of the imbalance can be mitigated, allowing the model to be trained more equitably across all classes and improving its ability to recognize and classify the minority classes.

### Traditional algorithms
After familiarizing oneself with the data, the next objective was to train the predictive model. For this purpose, a set of supervised learning algorithms will be selected and compared for each dataset. Given that this is a binary classification problem, the following algorithms have been chosen:
* Decision Tree (DT): This supervised learning algorithm constructs a decision tree from the training data. Each node in the tree represents a feature, each branch represents a decision based on that feature, and each leaf represents a classification label. It is useful for classification and regression problems, providing a clear and easy-to-understand visualization of the decision-making process.
* K-Nearest Neighbors (KNN): This classifier assigns a label to a data point based on the majority labels of its k nearest neighbors. It is a simple and versatile supervised learning algorithm that works well with datasets with non-linear structures and when locality in the data distribution is important. However, it can be computationally expensive for large datasets and may require proper normalization of the data to function correctly.
* Support Vector Classifier (SVC): SVC is a supervised learning algorithm that finds the optimal hyperplane that maximizes the margin between classes in the feature space. It is useful for binary classification problems and linearly separable data, but can also be used for non-linear classification and regression problems using kernels. SVM is robust to high-dimensional datasets and effective even with a small number of samples.
* XGBoost: XGBoost is a supervised learning algorithm based on decision trees that uses boosting techniques to improve predictive performance. It is highly effective for classification and regression problems and is known for its speed and performance with large and complex datasets. XGBoost also offers flexibility in hyperparameter tuning to optimize model performance.
For each algorithm, the GridSearchCV technique will be used to obtain the best hyperparameters according to the dataset being used. The following explains the hyperparameters that will be used for each algorithm:
* DT:
** criterion: Gini and Entropy are the criteria for measuring the quality of a split. Gini measures impurity, while Entropy measures the disorder of information.
** max_depth: Controls the maximum depth of the tree to avoid overfitting. None allows the tree to grow without restrictions, while values like 10, 20, and 30 limit its growth.
* KNN:
** n_neighbors: Number of neighbors to consider in classification. Testing values like 3, 5, and 7 helps find the balance between considering too few or too many neighbors.
** weights: Uniform gives equal weight to all neighbors, while distance gives more weight to closer neighbors.
** metric: Metric for measuring the distance between points. Euclidean and Manhattan are the options tested.
* SVC:
** C: Regularization parameter. A low C may cause underfitting, while a high C can lead to overfitting.
** kernel: Linear and rbf are types of kernels that transform the data. Linear is a linear transformation, while rbf is a non-linear transformation based on radial functions.
** gamma: Scale and auto are the options tested for the gamma coefficient in non-linear kernels.
* XGBoost:
* n_estimators: Number of trees to be built in the model. Tested values: 50, 100, 200.
* max_depth: Maximum depth of each tree. Tested values: 3, 6, 9.
* learning_rate: Learning rate, which controls the contribution of each tree to the final model. Tested values: 0.01, 0.1, 0.2.
* subsample: Proportion of samples used to train each tree. Tested values: 0.8, 1.0.
* colsample_bytree: Proportion of features used to train each tree. Tested values: 0.8, 1.0.
To evaluate different preprocessing options, the holdout method will be used, creating several partitions using the train-test algorithm. The dataset will be divided into 5 different partitions, controlling reproducibility with a seed. For each partition, GridSearchCV will be applied to perform hyperparameter tuning, using a pipeline that includes preprocessing and the supervised model.
The model performance will be evaluated using multiple metrics to provide a more comprehensive view of the models' predictive capabilities:
* Precision: Proportion of true positives among all instances classified as positive. High precision indicates that most of the model's positive predictions are correct.
* Recall: Proportion of true positives among all instances that are actually positive. High recall indicates that the model can identify most of the positive instances.
* F1-Score: The harmonic mean between precision and recall. It is especially useful in scenarios where the balance between precision and recall is crucial, such as in cases with imbalanced classes.
* AUC-ROC (Area Under the ROC Curve): Measures the model's ability to distinguish between positive and negative classes. The ROC curve shows the relationship between the True Positive Rate (TPR) and the False * Positive Rate (FPR) at various classification thresholds. The area under the ROC curve (AUC) provides a single value that measures the model's ability to distinguish between classes. An AUC close to 1 indicates excellent performance.
For each trained model, the F1 score obtained in each partition will be recorded, along with the mean and standard deviation of these scores, as well as for the Precision, Recall, and AUC-ROC metrics. This process ensures that the results are consistent and that the model generalizes well to different subsets of data.
The code also includes visualization of the confusion matrix and the ROC curve for each partition and each model, providing additional insights into the model's performance in classifying the classes.
Finally, although all models will be evaluated, the model trained with XGBoost will always be selected as the final model due to its effectiveness for analysis with interpretability algorithms.

### GAN Network
Patient samples with CVDs (Cardiovascular Diseases) present a class imbalance issue, with a higher number of patients without the risk of heart attack compared to those with the risk of it. Addressing this imbalance is crucial to prevent the model from exhibiting bias in its predictions. To address this issue, GAN (Generative Adversarial Network) models will be used to increase the number of samples of patients at risk of heart attack.
This architecture consists of a generative network aimed at producing new data samples indistinguishable from the original ones, accompanied by an adversarial network whose purpose is to discern between the real and synthetic data. In this way, both networks complement each other in training until the synthetic samples become virtually indistinguishable from the original ones.
Among all possible GAN architecture implementations to choose from, the CTGAN technique has been specifically selected. CTGAN is designed to handle the peculiarities of tabular data, making it particularly interesting to test its effectiveness in a case where all data samples are of tabular nature.
To use CTGAN in the notebook, the ctgan library has been installed and the CTGAN class has been imported. Additionally, the function balance_dataset_with_ctgan has been defined, which takes as input a DataFrame, the name of the target column, discrete columns, and the number of epochs for training the CTGAN model.
The function evaluates the class imbalance by counting the number of samples in each class. The majority class is identified, and the number of additional samples needed for each minority class is calculated to balance the dataset.
The CTGAN model is initialized and trained with the features (X) of the dataset. For each minority class, synthetic samples are generated, stored in a DataFrame, and labeled with the corresponding class. The generated synthetic samples are combined with the original dataset to form a new DataFrame.
Finally, the function returns the balanced DataFrame, which now has an equivalent number of samples for each class, improving the model’s ability to learn without bias towards the majority class.

### Transformer algorithm
Transformers are algorithms in the field of machine learning (ML) that use a mechanism called "self-attention," which allows the model to evaluate the importance of each word in a sentence relative to the others. This enables capturing long-term dependencies more effectively than recurrent neural networks (RNNs). For this work, transformer-based algorithms adapted to work with tabular data sources will be used, specifically the TabPFN model.
Tabular Pretrained Few-shot Network (TabPFN) is the transformer architecture chosen for this project. TabPFN combines the flexibility of transformers with ML algorithms and, like CTGAN, is specifically designed to work with tabular data, making it an excellent option to explore. TabPFN functions as a "few-shot learning" model for tabular data, which means it can make high-quality predictions with only a few data samples. This approach is very useful in situations where available data is scarce, reducing reliance on synthetic data and the need to obtain large private medical datasets.
Regarding code development, first, the tabpfn library needs to be installed. After that, the test_model_tabpfn function is defined, which evaluates the performance of the TabPFN model using the same metrics as traditional models.
The main difficulty lies in the evaluate_models_tabpfn function, which is responsible for creating an instance of the TabPFNClassifier model with its respective configurations, training it with the DataFrame information received, and testing it with the method defined in the previous paragraph. Finally, it returns the developed model for use by the algorithms defined in the next section.

### Interpretability
Once the predictive AI models based on transformers have been developed, interpretability processes will be applied to study how the model evaluates data when making predictions. This will allow not only predicting a patient's risk of heart disease but also explaining the causes and variables that led to that risk assessment.
In this regard, the following interpretability techniques will be applied to the model driven by the XGBoost algorithm obtained in previous sections:
SHapley Additive exPlanations (SHAP) is a technique that decomposes a model's prediction into individual contributions from each feature, providing a coherent and quantitative explanation of how each factor influenced the final prediction.
Local Interpretable Model-agnostic Explanations (LIME) is another technique that creates a locally interpretable model around a specific prediction to explain the behavior of a complex model. It does this by slightly modifying the input data to observe how predictions change, which helps identify which features were most influential for that particular prediction.
Regarding the code, it requires downloading the shap and lime libraries, which will be used in the interpret_model function. This function performs the analysis, starting by splitting the data into training and test sets, and then taking a sample of these data to optimize memory usage. With this sample, interpretation can proceed.
Firstly, SHAP is used to break down the model's predictions into individual contributions from each feature. Since the model is XGBoost, TreeExplainer is employed to calculate SHAP values, which is a specific alternative for tree-based models. These values allow visualization of how each feature influences the prediction globally through a summary plot, making it easy to identify which factors are most decisive in the model's predictions.
Secondly, LIME is implemented to obtain a local interpretation of the model. This is achieved using the LimeTabularExplainer function, which provides a detailed explanation for a specific prediction, visualized with a bar chart showing the importance of features for that particular instance.
Together, these techniques provide both global and local insights into the model's behavior, which is essential to ensure that predictions are not only accurate but also transparent and justifiable. This is particularly important in medical applications where understanding why a model has classified a patient with a certain level of risk is crucial.

## Conclusion
The objective that motivated this project was to help individuals understand the risk associated with their lifestyle by providing them with an AI-based assessment of their risk of developing a cardiovascular disease (CVD) due to their lifestyle habits and biological variables. Ultimately, the goal is to encourage individuals to consider this risk and improve their health habits.
On a scientific level, it is hoped that this work will contribute to the knowledge in the field of prevention and prediction of risk factors in CVDs and provide relevant information for predicting potential heart attacks in the near future based on clinical data and lifestyle habits of the patient.
Developing a minimum viable product for Cardiax has involved creating a wide variety of screens and components on the frontend, as well as multiple functionalities and endpoints on the backend. The AI models were created following a rigorous process that included all the usual stages in such developments, from data ingestion and exploratory analysis to preprocessing and correcting data imbalances, culminating in the training and evaluation of traditional and transformer models, which were subsequently interpreted.
Finally, I consider it significant to have experimented with technologies such as GAN models, transformer algorithms, and interpretability techniques. These have allowed us to offer an innovative approach to solving the problem and have achieved excellent results capable of rivaling conventional techniques.
The development of Cardiax has been one of the greatest technical challenges I have faced, as building the web platform from scratch—both frontend and backend—along with the communication between them and database management, required learning a wide range of languages, frameworks, and configuration tools.
I have been fascinated by learning about different techniques aimed at innovating and improving the stages of creating AI models, broadening my horizons and further satisfying my curiosity in this field. Additionally, it has allowed me to experience firsthand the continuous evolution in this field and its potential for further development.
In summary, I take great pride in contributing a small but meaningful addition to the field of health that I am so passionate about, through the creation of a software project where I could apply all my knowledge as a computer and software engineer, and future specialist in Artificial Intelligence.
